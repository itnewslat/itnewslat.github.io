---
layout: posts
color-schema: red-dark
date: '2025-05-22 15:46 -0400'
published: true
superNews: false
superArticle: false
year: '2025'
title: >-
  Red Hat optimiza Red Hat AI para acelerar las implementaciones de la IA
  empresarial
image: >-
  https://raw.githubusercontent.com/itnewslat/assets/refs/heads/master/img/540x320/AI-p.jpg
detail-image: >-
  https://raw.githubusercontent.com/itnewslat/assets/refs/heads/master/img/1024x680/AI-g.jpg
categories:
  - Venezuela
  - Colombia
  - Argentina
  - Perú
  - Ecuador
  - Chile
  - Panama
  - Mexico
tags:
  - Transformación Digital
week: '20'
---
**en todos los modelos, aceleradores de IA y nubes**

- Red Hat AI Inference Server, los modelos validados y la integración de Llama Stack y el Protocolo de Contexto de Modelo ayudan a los usuarios a proporcionar aplicaciones y agentes de IA más uniformes y de mayor rendimiento

Red Hat, el proveedor líder mundial de soluciones open source, continúa hoy ofreciendo a sus clientes alternativas de IA empresarial con la presentación de Red Hat AI Inference Server, modelos validados para Red Hat AI y la integración de las API de Llama Stack y el Protocolo de Contexto de Modelo (MCP), además de importantes actualizaciones en su portfolio de Red Hat AI. A través de estos desarrollos, Red Hat busca optimizar las capacidades que las empresas necesitan para acelerar la adopción de la IA, al mismo tiempo que ofrece a los clientes una mayor variedad y confianza en las implementaciones de producción de IA generativa (gen AI) en toda la nube híbrida.

Según Forrester, el software de código abierto será la clave que impulse las iniciativas de IA empresarial.1 Frente al panorama cada vez más complejo y dinámico de la IA, Red Hat AI Inference Server y los modelos validados de terceros proporcionan una inferencia de modelos eficiente y una colección de modelos de IA comprobados con rendimiento optimizado en la plataforma Red Hat AI. Además de la integración de nuevas API para el desarrollo de agentes de IA generativa, como Llama Stack y MCP, Red Hat trabaja para hacer frente a la complejidad de las implementaciones, facilitando que los líderes de TI, científicos de datos y desarrolladores aceleren las iniciativas de IA con mayor control y eficiencia.

**Red Hat AI Inference Server para una inferencia eficiente en toda la nube híbrida**

El portfolio de Red Hat AI ahora incluye el nuevo Red Hat AI Inference Server, que proporciona una inferencia más veloz, uniforme y de bajo costo a gran escala en entornos de nube híbrida. Esta importante incorporación viene incluida en las últimas versiones de Red Hat OpenShift AI y Red Hat Enterprise Linux AI (RHEL AI) y también está disponible como una solución autónoma, lo que permite a las empresas implementar aplicaciones inteligentes con mayor eficiencia, flexibilidad y rendimiento.

**Modelos probados y optimizados mediante modelos validados de terceros para Red Hat AI**

Los modelos validados de terceros para Red Hat AI, disponibles en Hugging Face, facilitan a las empresas la búsqueda de los modelos que mejor se adecuen a sus necesidades específicas. Red Hat AI ofrece una colección de modelos validados y guías de implementación para reforzar la confianza del cliente en el rendimiento del modelo y la facilidad de reproducción de los resultados. Red Hat también optimiza algunos modelos específicos mediante técnicas de compresión de modelos para reducir su tamaño y aumentar la velocidad de inferencia, lo que ayuda a minimizar el consumo de recursos y los costos operativos. Además, el proceso continuo de validación de modelos ayuda a los clientes de Red Hat AI a mantenerse a la vanguardia de la innovación en IA generativa optimizada.

**API estandarizadas para el desarrollo de aplicaciones y agentes de IA con Llama Stack y MCP**

Red Hat AI está integrando Llama Stack, inicialmente desarrollado por Meta, junto con MCP de Anthropic, para ofrecer a los usuarios API estandarizadas para la creación e implementación de aplicaciones y agentes de IA.  Llama Stack, disponible en Red Hat AI como versión preliminar para desarrolladores, proporciona una API unificada para acceder a la inferencia con vLLM, la generación aumentada por recuperación (RAG), la evaluación de modelos, barreras de seguridad y agentes en cualquier modelo de IA generativa. MCP permite integrar los modelos con herramientas externas al proporcionar una interfaz estandarizada que conecta las API, los plugins y las fuentes de datos en los flujos de trabajo de los agentes.

La última versión de Red Hat OpenShift AI (v2.20) incorpora mejoras adicionales para la creación, el entrenamiento, la implementación y el monitoreo de modelos de IA generativa e IA predictiva a gran escala. Estas mejoras incluyen las siguientes:
- **Catálogo de modelos optimizados (en versión tecnológica preliminar)**, que proporciona un fácil acceso a modelos validados de Red Hat y de terceros, permite su implementación en clústeres de Red Hat OpenShift AI a través de la interfaz de la consola web y gestiona su ciclo de vida por medio del registro integrado de Red Hat OpenShift AI.
- **Entrenamiento distribuido a través del Operador de Entrenamiento de KubeFlow**, que permite la programación y la ejecución del ajuste de modelos de InstructLab y otras cargas de trabajo de entrenamiento y ajuste basadas en PyTorch, distribuidas en múltiples nodos de Red Hat OpenShift y GPU, e incluye la aceleración de redes RDMA distribuidas y el aprovechamiento optimizado de GPU para reducir costos.
- **Tienda de características (en versión tecnológica preliminar)**, basado en el proyecto Kubeflow Feast, es un repositorio centralizado para gestionar y suministrar datos tanto para el entrenamiento como para la inferencia de modelos, que agiliza los flujos de trabajo de datos para mejorar la precisión y la reutilización de modelos.

Red Hat Enterprise Linux AI 1.5, que aporta nuevas actualizaciones a la plataforma de modelos base de Red Hat para desarrollar, probar y ejecutar modelos de lenguaje de gran tamaño (LLM). Las características clave de la versión 1.5 incluyen:
- **Disponibilidad de Google Cloud Marketplace**, que amplía las opciones del cliente para ejecutar RHEL AI en entornos de nube pública, junto con AWS y Azure, para ayudar a simplificar la implementación y la gestión de cargas de trabajo de IA en Google Cloud.
- **Funciones multilingües optimizadas para español**, alemán, francés e italiano a través de InstructLab, lo cual permite la personalización de modelos mediante scripts nativos y abre nuevas posibilidades para aplicaciones de IA multilingües.  Los usuarios también pueden traer sus propios modelos maestro y alumno para lograr un mayor control de la personalización y la prueba de modelos para casos de uso e idiomas específicos. En el futuro, se prevé la incorporación del japonés, el hindi y el coreano.

El servicio Red Hat AI InstructLab en IBM Cloud también goza de disponibilidad general.  Este nuevo servicio en la nube agiliza aún más el proceso de personalización de modelos, ya que mejora la escalabilidad y la experiencia del usuario y permite a las empresas hacer uso de sus datos particulares con mayor facilidad y control.

**La visión de Red Hat: cualquier modelo, acelerador o nube**

El futuro de la IA debe definirse por oportunidades ilimitadas, sin silos de infraestructura que la restrinjan. Red Hat visualiza un horizonte en el que las empresas puedan implementar cualquier modelo, en cualquier acelerador y en cualquier nube, al mismo tiempo que ofrecen una experiencia de usuario excepcional y más uniforme sin costos exorbitantes. Para aprovechar al máximo el verdadero potencial de las inversiones en IA generativa, las empresas necesitan una plataforma de inferencia universal, un estándar para lograr una innovación en IA más fluida y de alto rendimiento, tanto hoy como en el futuro.

**Citas de apoyo**

Joe Fernandes, vicepresidente y gerente general, Unidad de Negocio IA, Red Hat
“Una inferencia más rápida y eficiente se perfila como el nuevo punto de decisión para la innovación en IA generativa. Red Hat AI, con capacidades de inferencia optimizadas a través de Red Hat AI Inference Server y una nueva colección de modelos de terceros validados, ayuda a las empresas a implementar aplicaciones inteligentes donde las necesiten, en la forma en que las necesiten y con los componentes que mejor se adapten a sus requerimientos específicos”.

1 Fuente: “Navigate The Open-Source AI Ecosystem In The Cloud”, Forrester Research, Inc., febrero de 2025

![](https://raw.githubusercontent.com/itnewslat/assets/refs/heads/master/img/540x320/AI-p.jpg)

<table style="height: 42px;" width="569">
<tbody>
<tr>
<td style="text-align: justify;"><sub><strong>Nuestras noticias también son publicadas a través de nuestra cuenta en Twitter <a href="https://twitter.com/itnewslat?lang=es">@ITNEWSLAT</a> y en la aplicación <a href="https://squidapp.co/en/">SQUID</a></strong></sub></td>
</tr>
</tbody>
</table>

<img src="https://tracker.metricool.com/c3po.jpg?hash=56f88a41e39ab42c063cc51676587a04"/>