---
layout: posts
color-schema: red-dark
date: '2023-05-11 05:33 -0500'
published: true
superNews: false
superArticle: false
year: '2023'
title: 'Tu voz, tu vulnerabilidad'
detail-image: >-
  https://raw.githubusercontent.com/itnewslat/assets/master/img/1024x680/Vulnerabilidad-g.jpg
image: >-
  https://raw.githubusercontent.com/itnewslat/assets/master/img/540x320/Vulnerabilidad-p.jpg
categories:
  - Venezuela
  - Colombia
  - Argentina
  - Perú
  - Ecuador
  - Chile
  - Panama
  - Mexico
tags:
  - Seguridad
week: '19'
---
Nunca ha sido tan fácil cometer ciberdelitos. Los estafadores están usando tecnología de Inteligencia Artificial (IA) para clonar voces, replicar acentos de todo el mundo, entonaciones, modismos incluyendo formatos específicos según la geografía y luego enviar un mensaje de voz falso o llamar a los contactos de la víctima fingiendo estar angustiados, tal y como lo hacen luego de secuestrar tu cuenta de Whatsapp o Instagram suplantando identidad como viene ocurriendo desde el inicio de la pandemia.
 
Ni siquiera nos detenemos a pensarlo, pero escuchar la voz de una persona es suficiente para saber de quien se trata y depositar confianza. Hemos registrado un aumento en los ciberataques que aprovechan los sitios web asociados con ChatGPT, provocando distribución de malware e intentos de phishing a través de sitios web que parecen estar relacionados con la app. Ha habido varios ataques relacionados con la propagación de malware e intentos de phishing a través de sitios web que parecen estar asociados a ella. Después de hacer click, las víctimas son redirigidas a sitios web imitan a ChatGPT: chat-gpt-pc.online, chat-gpt-online-pc.com, chat-gpt-for-windows.com, chat-gpt-ai-pc.info, etc.
 
Ahora la IA está propiciando un aumento en las estafas de voz ON LINE, con solo cinco segundos de audio es suficiente para clonar la voz de una persona. Históricamente esta ha sido una estafa típica en transacciones bancarias que utilizaban “la voz” como método de autenticación. Los estafadores están usando tecnología de IA para clonar voces.
 
La voz de cada persona es única, el equivalente hablado de una huella dactilar biométrica, razón por la cual escuchar hablar a alguien es una forma tan ampliamente aceptada de generar confianza. El 50% de los adultos comparten su voz en formato digital, al menos, una vez a la semana, a través de las redes sociales, mensajes de voz, etc. Cerca del 30% lo hace 12 veces o más por semana.
 
Manipular imágenes, videos y las voces de compañeros de trabajo, amigos y familiares es cada vez más fácil. La tendencia es que la mayor probabilidad de convencimiento la tengan padres y madres cuando se trata del pedido de auxilio de un menor, relativo a: “tuve un accidente, perdí mi teléfono, me asaltaron, me quedé sin plata”.
 
Existe ya un gran número de herramientas gratuitas y de pago, y muchas requieren solo un nivel básico de conocimientos. En un solo caso, bastaron cinco segundos de audio como fuente original junto con varias desgravaciones para producir una semejanza altísima con la voz original, alrededor del 90%. Se trata de vulnerabilidades emocionales que apuntan a las relaciones cercanas.

![](https://raw.githubusercontent.com/itnewslat/assets/master/img/540x320/Vulnerabilidad-p.jpg)

<table style="height: 42px;" width="569">
<tbody>
<tr>
<td style="text-align: justify;"><sub><strong>Nuestras noticias también son publicadas a través de nuestra cuenta en Twitter <a href="https://twitter.com/itnewslat?lang=es">@ITNEWSLAT</a> y en la aplicación <a href="https://squidapp.co/en/">SQUID</a></strong></sub></td>
</tr>
</tbody>
</table>
<img src="https://tracker.metricool.com/c3po.jpg?hash=56f88a41e39ab42c063cc51676587a04"/>